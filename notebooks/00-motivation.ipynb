{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed and Accelerated Computing\n",
    "\n",
    "What happens when we run some Python code? Firstly, the code is compiled and turned into language the machine can understand, the code will then loaded within a Python virtual machine on the machine, finally the CPU will then execute the code step by step. When we execute code we generally want it to run as fast as possible, however we will always be bounded or limited in some aspect. Profiling our code is essential to understanding where this limitation is occuring, be it compute, memory, or i/o. When we understand what is limiting our code we can start to leverage strategies to reduce the bottleneck.\n",
    "\n",
    "In this workshop will explore how to analyze our code to understand our bottlenecks and discussing tolling and approaches to address them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bottlenecks\n",
    "Bottlenecks will always exist in our code otherwise it would run instantly.\n",
    "\n",
    "### Compute\n",
    "When executing code on the CPU the lines of code are executed sequentially, sometimes our operations simply take too long. In this scenario we may try to exploit some parallizable aspect of the computation to distrbute the computation across multiple compute units, either CPUs or GPUs.\n",
    "\n",
    "We would refer to this as distributing the computation across multiple CPU cores or even multiple machines with multiple CPUs. We would refer to vectorizing the computation on the GPU or some other application specific integratic circuit (ASIC) as accelerated computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of a job that might benefit from parallelization as each computation is independent of the other computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# define some computation which takes a long time\n",
    "def computation(x):\n",
    "    time.sleep(1.0)\n",
    "    return x ** 2\n",
    "\n",
    "# defin the inputs and outputs\n",
    "inputs = [1, 2, 3]\n",
    "outputs = []\n",
    "\n",
    "# perform the computation one by one\n",
    "for val in inputs:\n",
    "    output = computation(val)\n",
    "    outputs.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribute the computation ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of a job that might benefit from vectorization, rather than iterating over each value we can structure the computation so that we perform all of them at the same time. This is what GPUs in particular are very good at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sz = 100\n",
    "A = np.random.randn(sz,sz)\n",
    "B = np.random.randn(sz,sz)\n",
    "\n",
    "\n",
    "def matrix_multiply(A, B):\n",
    "    # Create the result matrix with zeros\n",
    "    output = np.zeros((A.shape[0], B.shape[1]))\n",
    "    \n",
    "    # Perform matrix multiplication\n",
    "    for i in range(A.shape[0]):\n",
    "        for j in range(B.shape[1]):\n",
    "            output[i][j] = sum(A[i][k] * B[k][j] for k in range(A.shape[1]))\n",
    "    \n",
    "    return output\n",
    "\n",
    "# perform the computation\n",
    "output = matrix_multiply(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform vectorized matrix multiplication\n",
    "output = np.matmul(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory\n",
    "Another bottleneck we may encounter is memory where the data or intermediaries of a computation is simply too large to store in memory. In this scenario we may try to break the computation up into more manageable chunks to process individually.\n",
    "\n",
    "We would refer to this as chunking and often we might decide to distribute the computation being performed on each of these chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "Path(\"..\").joinpath(\"data\").mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider you're trying to read in a massive CSV file containing billions of transactions and you want to compute the net value of the transactions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "N = 1000000\n",
    "lines = {\n",
    "    \"user\": [random.randint(0,10) for _ in range(N)],\n",
    "    \"transaction_value\": [100 * random.random() for _ in range(N)]\n",
    "}\n",
    "pd.DataFrame(lines).to_csv(\"../data/huge_file.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(50026984.48608181)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# say this doesnt fit into memory\n",
    "df = pd.read_csv(\"../data/huge_file.csv\") # e.g. 100TiB of memory?!\n",
    "\n",
    "\n",
    "df[\"transaction_value\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading this entire file into memory at once would use up more memory than your machine has available. If we can't load the data does that mean we can't do the computation at all?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(50026984.4860818)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_sum = 0\n",
    "rows_per_chunk = 1000\n",
    "\n",
    "# process the file chunk by chunk\n",
    "for chunk in pd.read_csv(\"../data/huge_file.csv\", chunksize=rows_per_chunk):\n",
    "    # process each chunk independently : QUICK!\n",
    "    chunk_sum += chunk[\"transaction_value\"].sum() \n",
    "    \n",
    "chunk_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I/O\n",
    "When reading/writing to the filesystem we can also be limited by the I/O speed. In this scenario we may try to parralelize the reading/writing to saturate the I/O bandwidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do we need to process the files one at a time in they're independent?\n",
    "for file in [\"../data/huge_file.csv\"] * 10:\n",
    "    df = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribute the computation ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute -> Parallelize, Vectorize\n",
    "### Memory -> Chunk\n",
    "### IO -> Parallelize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
